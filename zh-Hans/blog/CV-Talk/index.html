<!doctype html>
<html lang="zh-Hans" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">有关视觉识别领域发展的观点 | SeekYou</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://cq.seekyou.fun/zh-Hans/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://cq.seekyou.fun/zh-Hans/img/social-card.png"><meta data-rh="true" property="og:url" content="https://cq.seekyou.fun/zh-Hans/blog/CV-Talk"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="有关视觉识别领域发展的观点 | SeekYou"><meta data-rh="true" name="description" content="先对24年华为天才少年谢凌曦的讨论再总结一下（文末括号表示个人的想法）， 后续或许更新自己的一些想法/ideas。"><meta data-rh="true" property="og:description" content="先对24年华为天才少年谢凌曦的讨论再总结一下（文末括号表示个人的想法）， 后续或许更新自己的一些想法/ideas。"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-05-30T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/cowqer"><meta data-rh="true" property="article:tag" content="Note,seekyou"><link data-rh="true" rel="icon" href="/zh-Hans/img/yanqing.ico"><link data-rh="true" rel="canonical" href="https://cq.seekyou.fun/zh-Hans/blog/CV-Talk"><link data-rh="true" rel="alternate" href="https://cq.seekyou.fun/blog/CV-Talk" hreflang="en"><link data-rh="true" rel="alternate" href="https://cq.seekyou.fun/zh-Hans/blog/CV-Talk" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://cq.seekyou.fun/blog/CV-Talk" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/zh-Hans/blog/rss.xml" title="SeekYou RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zh-Hans/blog/atom.xml" title="SeekYou Atom Feed">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css"><link rel="stylesheet" href="/zh-Hans/assets/css/styles.a0cb5927.css">
<script src="/zh-Hans/assets/js/runtime~main.5063b709.js" defer="defer"></script>
<script src="/zh-Hans/assets/js/main.c0ba1fae.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳到主要"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zh-Hans/"><div class="navbar__logo"><img src="/zh-Hans/img/yanqing.svg" alt="Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/zh-Hans/img/yanqing.svg" alt="Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">觅友</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zh-Hans/blog">博客</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">More</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/zh-Hans/docs/skill">Note</a></li><li><a class="dropdown__link" href="/zh-Hans/docs/all-skill-intro">test</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>简体中文</a><ul class="dropdown__menu"><li><a href="/blog/CV-Talk" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/zh-Hans/blog/CV-Talk" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-Hans">简体中文</a></li></ul></div><a class="navbar__item navbar__link" href="/zh-Hans/introduce">About</a><a href="https://github.com/cowqer" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="最近博客导航"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/zh-Hans/blog/CV-Talk">有关视觉识别领域发展的观点</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zh-Hans/blog/CV-Class-report">计算机视觉报告</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zh-Hans/blog/shareing">竞赛经验分享</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zh-Hans/blog/beijing">天智杯比赛</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zh-Hans/blog/wuhan">地平线智能车赛</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="https://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="先对24年华为天才少年谢凌曦的讨论再总结一下（文末括号表示个人的想法）， 后续或许更新自己的一些想法/ideas。"><header><h1 class="title_f1Hy" itemprop="headline">有关视觉识别领域发展的观点</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-05-30T00:00:00.000Z" itemprop="datePublished">2025年5月30日</time> · <!-- -->阅读需 41 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/cowqer" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/u/102907373?v=4" alt="Cui quan" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/cowqer" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Cui quan</span></a></div><small class="avatar__subtitle" itemprop="description">Maintainer of site</small></div></div></div></div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><p>先对24年华为天才少年谢凌曦的讨论再总结一下（文末括号表示个人的想法）， 后续或许更新自己的一些想法/ideas。</p>
<p><a href="https://mp.weixin.qq.com/s/rF5mREJerWfg1rf_Gm9RCQ" target="_blank" rel="noopener noreferrer">原文link</a></p>
<p>在我看来其思路是很十分正确的，或许有些话会一语成谶。</p>
<p>主要对分析计算机视觉领域，特别是<strong>视觉感知（即识别）方向所面临的困难和潜在的研究方向</strong>。</p>
<p>相较于针对具体算法的细节改进，谢更希望探讨当前算法（尤其是基于深度学习的预训练+微调范式）的局限性和瓶颈，并且由此得出初步的发展性结论，包括<strong>哪些问题是重要的、哪些问题是不重要的、哪些方向值得推进、哪些方向的性价比较低等</strong>。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cv-和-nlp">CV 和 NLP<a href="#cv-和-nlp" class="hash-link" aria-label="CV   和 NLP的直接链接" title="CV 和 NLP的直接链接">​</a></h2>
<p><a href="https://img.tg/image/Odjp3F" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://ooo.0x0.ooo/2025/05/30/Odjp3F.md.webp" alt="Odjp3F.md.webp" class="img_ev3q"></a></p>
<p>简单翻译一下：</p>
<p>NLP：处理主观的文本数据</p>
<ol>
<li>语义密集性：语言是“高效存储信息的载体”</li>
<li>较小的域差异：不管是哪种语言，本质的任务仍然是一样的</li>
<li>有限的细粒度：语言有单位（词），定义了最初的语义</li>
</ol>
<p>CV：处理客观的图像数据</p>
<ol>
<li>语义稀疏性：同样包含了简单的语义，但是需要大量的像素存储</li>
<li>较大域差异：（训练和测试）数据分布上的区别可能很大</li>
<li>无尽的语义（细粒度）：仅仅靠人为的标签不足以表示清楚图片的全部信息</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cv的三大基本困难和对应研究方向">CV的三大基本困难和对应研究方向<a href="#cv的三大基本困难和对应研究方向" class="hash-link" aria-label="CV的三大基本困难和对应研究方向的直接链接" title="CV的三大基本困难和对应研究方向的直接链接">​</a></h3>
<p>一直以来，NLP都走在CV的前面。不论是深度神经网络超越手工方法，还是预训练大模型开始出现大一统的趋势，这些事情都先发生在NLP领域，并在不久之后被搬运到了CV领域。这里的本质原因是NLP的起点更高：自然语言的基础单元是单词，而图像的基础单元是像素；前者具有天然的语义信息，而后者未必能够表达语义。</p>
<p>从根本上说，自然语言是人类创造出来，用于存储知识和交流信息的载体，所以必然具有高效和信息密度高的特性；而图像则是人类通过各种传感器捕捉的光学信号，它能够客观地反映真实情况，但相应地就不具有强语义，且信息密度可 能很低。</p>
<p>从另一个角度看，图像空间比文本空间要大得多，空间的结构也要复杂得多。这就意味着，如果希望在空间中采样大量样本，并且用这些数据来表征整个空间的分布，采样的图像数据就要比采样的文本数据大许多个数量级。顺带一提，这也是为什么自然语言预训练模型比视觉预训练模型用得更好的本质原因——我们在后面还会提到这一点。</p>
<p>根据上述分析，我们已经通过CV和NLP的差别，引出了CV的第一个基本困难，即语义稀疏性。而另外两个困难，域间差异性和无限粒度性，也多少与上述本质差别相关。正是由于图像采样时没有考虑到语义，因而在采样不同域（即不同分布，如白天和黑夜、晴天和雨天等场景）时，采样结果（即图像像素）与域特性强相关，导致了域间差异性。同时，由于图像的基本语义单元很难定义（而文本很容易定义），且图像所表达的信息丰富多样，使得人类能够从图像中获取近乎无限精细的语义信息，远远超出当前CV领域任何一种评价指标所定义的能力，这就是无限粒度性。关于无限粒度性，谢曾经写过一篇文章，专门讨论这个问题。</p>
<ul>
<li>
<p><strong>语义稀疏性</strong> ：解决方案为<strong>构建高效计算模型（神经网络）和视觉预训练</strong>。此处的主要逻辑在于，想要提升数据的信息密度，就必须假设数据的非均匀分布（信息论）并对其建模（即学习数据的先验分布）。目前，最为高效的建模方式有两类，一类是通过神经网络架构设计，来捕捉数据无关的先验分布（例如卷积模块对应于图像数据的局部性先验、transformer模块对应于图像数据的注意力先验）；一类是通过在大规模数据上的预训练，来捕捉数据相关的先验分布。这两个研究方向，也是视觉识别领域最为基础、受到关注最多的研究方向。
（但是个人认为这个任务是做不完全的，无穷无尽的，不论是算力和数据上的需求，还是算法的设计，该存在的domain gap任然存在。只能尽量去做一个近似的建模）</p>
</li>
<li>
<p><strong>域间差异性</strong>：解决方案为<strong>数据高效的微调算法</strong>。根据以上分析，网络体量越大、预训练数据集体量越大，计算模型中存储的先验就越强。然而，当预训练域和目标域的数据分布具有较大差异时，这种强先验反而会带来坏处，因为信息论告诉我们：提升某些部分（预训练域）的信息密度，就一定会降低其他部分（预训练域没有包含的部分，即预训练过程中认为不重要的部分）的信息密度。现实中，目标域很可能部分或者全部落在没有包含的部分，导致直接迁移预训练模型的效果很差（即过拟合）。此时，就需要通过在目标域进行微调来适应新的数据分布。考虑到目标域的数据体量往往远小于预训练域，因而数据高效是必不可少的假设。此外，从实用的角度看，模型必须能够适应随时变化的域，因而<strong>终身学习是必须</strong>。</p>
</li>
<li>
<p><strong>无限粒度性</strong>：解决方案为<strong>开放域识别算法</strong>。无限粒度性包含开放域特性，是更高的追求目标。这个方向的研究还很初步，特别是业界还没有能被普遍接受的开放域识别数据集和评价指标。这里最本质的问题之一，是<strong>如何向视觉识别中引入开放域能力</strong>。可喜的是，随着跨模态预训练方法的涌现（特别是2021年的CLIP），自然语言越来越接近成为开放域识别的牵引器，谢相信这会是未来2-3年的主流方向。然而，谢并不赞成在追求开放域识别的过程中，涌现出的各种zero-shot识别任务。谢认为zero-shot本身是一个伪命题，世界上并不存在也不需要zero-shot识别方法。现有的zero-shot任务，都是使用不同方法，将信息泄露给算法，而泄露方式的千差万别，导致不同方法之间难以进行公平对比。在这个方向上，谢提出了一种被称为按需视觉识别的方法，以进一步揭示、探索视觉识别的无限粒度性。</p>
</li>
</ul>
<p>这里需要做一个补充说明。由于数据空间大小和结构复杂度的差异，至少到目前为止，CV领域还不能通过预训练模型直接解决域间差异的问题，但是NLP领域已经接近了这一点。因此，我们看到了NLP学者们利用prompt-based方法统一了几十上百种下游任务，但是同样的事情在CV领域并没有发生。另外，在NLP中提出来的scaling law，其本质在于使用更大的模型来过拟合预训练数据集。也就是说，对于NLP来说，过拟合已经不再是一个问题，因为预训练数据集配合小型prompt已经足以表征整个语义空间的分布。但是，CV领域还没有做到这一点，因此还需要考虑域迁移，而域迁移的核心在于避免过拟合。也就是说，在接下来2-3年，CV和NLP的研究重心会有很大的差异，因而将任何一个方向的思维模式生搬硬套在另一个方向上，都是很危险的。</p>
<p>（的确没错，两者的研究中心已经开始偏移了。mamba的出现其实可以看出来，应用于NLP主要是为了隐式状态更新 + 时序结构的递推表达能力，而在CV中则更像是Tansformer的上位替代）</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cv的研究方向分析">CV的研究方向分析<a href="#cv的研究方向分析" class="hash-link" aria-label="CV的研究方向分析的直接链接" title="CV的研究方向分析的直接链接">​</a></h3>
<ul>
<li>
<p>2012年的AlexNet，奠定了深度神经网络在CV领域的基础。随后10年（至今），神经网络架构设计，经历了从手工设计到自动设计，再回到手工设计（引入更复杂的计算模块  ）的过程：</p>
</li>
<li>
<p>2012-2017年，手工构建更深的卷积神经网络，探索一般优化技巧。关键词：ReLU、Dropout、3x3卷积、BN、跳跃连接，等。在这个阶段，卷积操作是最基本的单元，它对应于图像特征的局部性先验。</p>
</li>
<li>
<p>2017-2020年，自动构建更复杂的神经网络。其中，网络架构搜索（NAS）盛行一时，最后定型为基础工具。在任意给定的搜索空间中，自动设计都能够达到稍微更好的结果，且能够快速适配不同的计算开销。</p>
</li>
<li>
<p>2020年至今，起源于NLP的transformer模块从被引入CV，利用attention机制，补足了神经网络的远距离建模能力。如今，大部分视觉任务的最优结果，都借助于包含transformer的架构所达到。</p>
</li>
</ul>
<p>对于这一方向的未来，谢的判断如下：</p>
<ol>
<li>
<p>如果视觉识别任务没有明显改变，那么不论是自动设计，或者加入更复杂的计算模块，都无法将CV推向新的高度。视觉识别任务的可能改变，大致可以分为输入和输出两个部分。输入部分的可能改变如event camera，它可能会改变规则化处理静态或者时序视觉信号的现状，催生特定的神经网络结构；输出部分的可能改变，则是某种统一各种识别任务的框架（方向3会谈到），它有可能让视觉识别从独立任务走向大一统，从而催生出一种更适合视觉prompt的网络架构。<strong>（即将特点的任务转为条件生成问题，根据任务的描述去解决问题）</strong></p>
</li>
<li>
<p>如果一定要在卷积和transformer之间做取舍，那么transformer的潜力更大，主要因为它能够统一不同的数据模态，尤其是文本和图像这两个最常见也最重要的模态。</p>
</li>
<li>
<p>可解释性是一个很重要的研究方向，但是谢个人对于深度神经网络的可解释性持悲观态度。NLP的成功，也不是建立在可解释性上，  而是建立在过拟合大规模语料库上。对于真正的AI来说，这可能不是太好的信号。</p>
</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="方向1视觉预训练">方向1：视觉预训练<a href="#方向1视觉预训练" class="hash-link" aria-label="方向1：视觉预训练的直接链接" title="方向1：视觉预训练的直接链接">​</a></h4>
<p>作为如今CV领域炙手可热的方向，预训练方法被寄予厚望。在深度学习时代，视觉预训练可以分为<strong>有监督、无监督、跨模态</strong>三类，大致叙述如下：</p>
<p>有监督预训练的发展相对清晰。由于图像级分类数据最容易获取，因此早在深度学习爆发之前，就有了日后奠定深度学习基础的ImageNet数据集，并被沿用至今。ImageNet全集超过1500万的数据规模，至今没有被其他非分类数据集所超越，因此至今仍是有监督预训练上最常用的数据。另外一个原因，则是图像级分类数据引入了较少bias，因而对于下游迁移更加有利——进一步减少bias，就是无监督预训练。</p>
<p>无监督预训练，则经历了曲折的发展历程。从2014年开始，出现了第一代基于几何的无监督预训练方法，如根据patch位置关系、根据图像旋转等进行判断，同时生成式方法也在不断发展（生成式方法可以追溯到更早的时期，此处不赘述）。此时的无监督预训练方法，还显著地弱于有监督预训练方法。到了2019年，<strong>对比学习</strong>方法经过技术改进，首次显现出在下游任务上超越有监督预训练方法的潜力，无监督学习真正成为CV界关注的焦点。而2021年开始，视觉transformer的兴起催生了一类特殊的<strong>生成式任务即MIM</strong>，它逐渐成为统治性方法。</p>
<p>除了纯粹的有监督和无监督预训练，还有一类介于两者之间的方法，是<strong>跨模态预训练</strong>。它使用弱配对的图像和 文本作为训练素材，一方面<strong>避免了图像监督信号带来的bias</strong>，一方面又<strong>比无监督方法更能学习弱语义</strong>。此外，在transformer的加持下，视觉和自然语言的融合也更自然、更合理。</p>
<p><strong>基于上述回顾，谢做出如下判断：</strong></p>
<p>从实际应用上看，应该将不同的<strong>预训练任务结合起来</strong>。也就是说，应当收集混合数据集，其中包含少量有标签数据（甚至是检测、分割等更强的标签）、中量图文配对数据、大量无任何标签的图像数据，并且在这样的混合数据集上设计预训练方法。</p>
<p>从CV领域看，<strong>无监督预训练是最能体现视觉本质的研究方向</strong>。即使跨模态预训练给整个方向带来了很大的冲击，谢依然认为无监督预训练非常重要，必须坚持下去。需要指出，视觉预训练的思路很大程度上受到了自然语言预训练的影响，但是两者性质不同，因而不能一概而论。尤其是，自然语言本身是人类创造出来的数据，其中每个单词、每个字符都是人类写下来的，天然带有语义，因此从严格意义上说，NLP的预训练任务不能被视为真正的无监督预训练，至多算是弱监督的预训练。但是视觉不同，图像信号是客观存在、未经人类处理的原始数据，在其中的无监督预训练任务一定更难。<strong>总之，即使跨模态预训练能够在工程上推进视觉算法，使其达到更好的识别效果，视觉的本质问题还是要靠视觉本身来解决</strong>。</p>
<p>当前，<strong>纯视觉无监督预训练的本质在于从退化中学习</strong>。这里的退化，指的是从图像信号中去除某些已经存在的信息，要求算法复原这些信息：几何类方法去除的是几何分布信息（如patch的相对位置关系）；对比类方法去除的是图像的整体信息（通过抽取不同的view）；  生成类方法如MIM去除的是图像的局部信息。这种基于退化的方法，都具有一个无法逾越的瓶颈，<strong>即退化强度和语义一致性的冲突</strong>。由于没有监督信号，视觉表征学习完全依赖于退化，因此退化必须足够强；而退化足够强时，就无法保证退化前后的图像具有语义一致性，从而导致病态的预训练目标。举例说，对比学习从一张图像中抽取的两个view如果毫无关系，拉近它们的特征就不合理；MIM任务如果去除了图像中的关键信息（如人脸），重建这些信息也不合理。强行完成这些任务，就会引入一定的bias，弱化模型的泛化能力。<strong>未来，应该会出现一种无需退化的学习任务，而其个人相信，通过压缩来学习是一条可行的路线</strong>。（压缩后再还原吗，有意思，可以试试）</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="方向2模型微调和终身学习">方向2：模型微调和终身学习<a href="#方向2模型微调和终身学习" class="hash-link" aria-label="方向2：模型微调和终身学习的直接链接" title="方向2：模型微调和终身学习的直接链接">​</a></h4>
<p>作为一个基础问题，模型微调已经发展出了大量的不同的setting。如果要把不同的setting统一起来，可以认为它们无非考虑三个数据集，即<strong>预训练数据集 Dpre （不可见）、目标训练集 Dtrain 、目标测试集 Dtest （不可见且不可预测）</strong>。根据对三者之间关系的假设不同，比较流行的setting可以概括如下：</p>
<p>迁移学习：假设Dpre或者 Dtrain 和 Dtest 的数据分布大不相同；</p>
<p>弱监督学习：假设Dtrain只提供了不完整的标注信息；</p>
<p>半监督学习：假设Dtrain只有部分数据被标注；</p>
<p>带噪学习：假设Dtrain的部分数据标注可能有误；</p>
<p>主动学习：假设Dtrain可以通过交互形式标注（挑选其中最难的 样本）以提升标注效率；</p>
<p>持续学习：假设不断有新的 Dtrain 出现，从而学习过程中可能会遗忘从 Dpre 学习的内容；</p>
<p>从一般意义上说，很难找到统一的框架来分析模型微调方法的发展和流派。从工程和实用角度看，模型微调的<strong>关键在于对域间差异大小的事先判断</strong>。如果认为 Dpre 和 Dtrain 的差异可能很大，就要减少从预训练网络中迁移到目标网络中权重的比例，或者增加一个专门的head来适应这种差异；如果认为 Dtrain 和 Dtest 的差异可能很大，就要在微调过程中加入更强的正则化以防止过拟合，或者在测试过程中引入某种在线统计量以尽量抵消差异。至于上述各种setting，则分别有大量研究工作，针对性很强，此处不再赘述。</p>
<p>关于这个方向，<strong>谢</strong>认为有两个重要问题：</p>
<p>**从孤立的setting向终身学习的统一。**从学术界到工业界，必须抛弃“一次性交付模型”的思维，将交付内容理解为以模型为中心，配套有数据治理、模型维护、模型部署等多种功能的工具链。用工业界的话说，一个模型或者一套系统，在整个项目的生命周期中，必须得到完整的看护。必须考虑到，用户的需求是多变且不可预期的，今天可能会换个摄像头，明天可能会新增要检测的目标种类，等等。我们不追求AI能自主解决所有问题，但是AI算法应该有一个规范操作流程，让不懂AI的人能够遵循这个流程，新增他们想要的需求、解决平时遇到的问题，这样才能让AI真正平民化，解决实际问题。<strong>对于学术界，必须尽快定义出符合真实场景的终身学习setting，建立起相应的benchmark，推动这一方向的研究</strong>。（很有意思的一点：结合到了工业界。很前卫的思路，可以理解为目前的chatgpt和以往的搜素引擎的差异）</p>
<p><strong>  在域间差异明显的情况下，解决大数据和小样本的冲突</strong>。这又是CV和NLP的不同点：NLP已经基本不用考虑预训练和下游任务的域间差异性，因为语法结构和常见单词完全一样；而CV则必须假设上下游数据分布显著不同，以致于上游模型未经微调时，在下游数据中无法抽取底层特征（被ReLU等单元直接滤除）。因此，用小数据微调大模型，在NLP领域不是大问题（现在的主流是只微调prompt），但是在CV领域是个大问题。在这里，<strong>设计视觉友好的prompt也许是个好方向，但是目前的研究还没有切入核心问题</strong>。（什么是视觉友好的prompt，了解了解）</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="方向3无限细粒度视觉识别任务">方向3：无限细粒度视觉识别任务<a href="#方向3无限细粒度视觉识别任务" class="hash-link" aria-label="方向3：无限细粒度视觉识别任务的直接链接" title="方向3：无限细粒度视觉识别任务的直接链接">​</a></h4>
<p>关于无限细粒度视觉识别（以及类似的概念），目前还没有很多相关的研究。所以，谢以自己的思路来叙述这个问题。我在今年VALSE报告上，对已有方法和我们的proposal做了详细解读。以下我给出文字叙述，更详细的解读请参考我的专题文章或者我在VALSE上做的报告：</p>
<p><a href="https://zhuanlan.zhihu.com/p/546510418" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/546510418</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/555377882" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/555377882</a></p>
<p>首先，我要阐述无限细粒度视觉识别的含义。简单地说，图像中包含的语义信息非常丰富，但不具有明确的基本语义单元。只要人类愿意，就可以从一张图像中识别出越来越细粒度的语义信息（如下图所示）；而这些信息，很难通过有限而规范的标注（即使花费足够多的标注成本），形成语义上完整的数据集，供算法学习。</p>
<p><a href="https://img.tg/image/OdjRtP" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://ooo.0x0.ooo/2025/05/30/OdjRtP.md.webp" alt="OdjRtP.md.webp" class="img_ev3q"></a></p>
<p>我们认为，无限细粒度视觉识别是比开放域视觉识别更难，也更加本质的目标。我们调研了已有识别方法，将其分为两类，即<strong>基于分类的方法和语言驱动</strong>的方法，并论述它们无法实现无限细粒度的理由。</p>
<p>基于分类的方法：这包括传统意义上的分类、检测、分割等方法，其基本特点是给图像中的每个基本语义单元（图像、box、mask、keypoint等）赋予一个类别标签。这种方法的致命缺陷在于，当识别的粒度增加时，识别的确定性必然下降，也就是说，<strong>粒度和确定性是冲突</strong>的。举例说，在ImageNet中，存在着“家具”和“电器”两个大类；显然“椅子”属于“家具”，而“电视机”属于“家电”，但是“按摩椅”属于“家具”还是“家电”，就很难判断——这就是语义粒度的增加引发的确定性的下降。如果照片里有一个分辨率很小的“人”，强行标注这个“人”的“头部”甚至“眼睛”，那么不同标注者的判断可能会不同；但是此时，即使是一两个像素的偏差，也会大大影响IoU等指标——这就是空间粒度的增加引发的确定性的下降。（即语义的必要性，很多时候某一些信息是冗余的，或者说是可有可无的，但是在粒度过高的前提下，这种冗余的信息会产生模型产生混淆。我个人任务认为细粒度识别不是更精确的分类问题，而是更复杂的语义建模问题。当标签空间无法自然地捕捉语义连续性与模糊性时，传统分类范式就失去了稳定性。）</p>
<p>语言驱动的方法：这包括CLIP带动的视觉prompt类方法，以及存在更长时间的visual grounding问题等，其基本特点是利用语言来指代图像中的语义信息并加以识别。语言的引入，确实增强了识别的灵活性，并带来了天然的开放域性质。然而语言本身的指代能力有限（想象一下，在一个具有上百人的场景中指代某个特定个体），无法满足无限细粒度视觉识别的需要。归根结底，在视觉识别领域，语言应当起到辅助视觉的作用，而已有的视觉prompt方法多少有些喧宾夺主的感觉。</p>
<p>上述调研告诉我们，当前的视觉识别方法并不能达到无限细粒度的目标，而且在走向无限细粒度的路上还会遭遇难以逾越的困难。因此，我们我们想分析人是如何解决这些困难的。首先，人类在大多数情况下并不需要显式地做分类任务：回到上述例子，一个人到商场里买东西，不管商场把“按摩椅”放在“家具”区还是“家电”区，人类都可以通过简单的指引，快速找到“按摩椅”所在的区域。其次，人类并不仅限于用语言指代图像中的物体，可以使用更灵活的方式（如用手指向物体）完成指代，进而做更细致的分析。</p>
<p>结合这些分析，要达到无限细粒度的目标，必须满足以下三个条件。</p>
<ol>
<li>
<p>开放性：开放域识别，是无限细粒度识别的一个子目标。目前看，引入语言是实现开放性的最佳方案之一。</p>
</li>
<li>
<p>特异性：引入语言时，不应被语言束缚，而应当设计视觉友好的指代方案（即识别任务）。</p>
</li>
<li>
<p>可变粒度性：并非总是要求识别到最细粒度，而是可以根据需求，灵活地改变识别的粒度。</p>
</li>
</ol>
<p>在这三个条件的牵引下，我们设计出了<strong>按需视觉识别任务</strong>。与传统意义上的统一视觉识别不同，按需视觉识别以request为单位进行标注 、学习和评测。当前，系统支持两类request，分别实现了从instance到semantic的分割、以及从semantic到instance的分割，因而两者结合起来，就能够实现任意精细程度的图像分割。按需视觉识别的另一个好处在于，在完成任意数量的request之后停止下来，都不会影响标注的精确性（即使大量信息没有被标注出来），这对于开放域的可扩展性（如新增语义类别）有很大的好处。具体细节，可以参看按需视觉识别的文章（链接见上文）。</p>
<p><a href="https://img.tg/image/Odjt2b" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://ooo.0x0.ooo/2025/05/30/Odjt2b.md.webp" alt="Odjt2b.md.webp" class="img_ev3q"></a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="统一视觉识别和按需视觉识别的对比"><strong>统一视觉识别和按需视觉识别的对比</strong><a href="#统一视觉识别和按需视觉识别的对比" class="hash-link" aria-label="统一视觉识别和按需视觉识别的对比的直接链接" title="统一视觉识别和按需视觉识别的对比的直接链接">​</a></h3>
<p>在完成这篇文章之后，我还在不断思考，按需视觉识别对于其他方向的影响是什么。这里提供两个观点：</p>
<p><strong>按需视觉识别中的request，本质上是一种视觉友好的prompt</strong>。它既能够达到询问视觉模型的目的，又避免了纯语言prompt带来的指代模糊性。随着更多类型的request被引入，这个体系有望更加成熟。</p>
<p><strong>按需视觉识别，提供了在形式上统一各种视觉任务的可能性</strong>。例如，分类、检测、分割等任务，在这一框架下得到了统一。这一点可能对视觉预训练带来启发。目前，视觉预训练和下游微调的边界并不清楚，预训练模型究竟应该适用于不同任务，还是专注于提升特定任务，尚无定论。然而，如果出现了形式上统一的识别任务 ，那么这个争论也许就不再重要。顺便说，下游任务在形式上的统一，也是NLP领域享有的一大优势。</p>
<p>在上述方向之外
我将CV领域的问题分为三大类：识别、生成、交互，识别只是其中最简单的问题。关于这三个子领域，简要的分析如下：</p>
<p>在识别领域，传统的识别指标已经明显过时，因此人们需要更新的评价指标。目前，在视觉识别中引入自然语言，是明显且不可逆的趋势，但是这样还远远不够，业界需要更多任务层面的创新。</p>
<p>生成是比识别更高级的能力。人类能够轻易地识别出各种常见物体，但是很少有人能够画出逼真的物体。从统计学习的语言上说，这是因为生成式模型需要对联合分布 p(x,y) 进行建模，而判别式模型只需要对条件分布 p(y|x) 进行建模：前者能够推导出后者，而后者不能推导出前者。从业界的发展看，虽然图像生成质量不断提升，但是生成内容的稳定性（不生成明显非真实的内容）和可控性仍有待提升。同时，生成内容对于识别算法的辅助还相对较弱，人们还难以完全利用虚拟数据、合成数据，达到和真实数据训练相媲美的效果。对于这两个问题，我们的观点都是，需要设计更好、更本质的评价指标，以替代现有的指标（生成任务上替代FID、IS等，而生成识别任务需要结合起来，定义统一的评价指标）。</p>
<p>1978年，计算机视觉先驱David Marr设想，视觉的主要功能，在于建立环境的三维模型，并且在交互中学习知识。相比于识别和生成，交互更接近人类的学习方式，但是现在业界的研究相对较少。交互方向研究的主要困难，在于构建真实的交互环境——准确地说，当前视觉数据集的构建方式来源于对环境的稀疏采样，但交互需要连续采样。显然，要想解决视觉的本质问题，交互是本质。虽然业界已经有了许多相关研究（如具身智能），但是还没有出现通用的、任务驱动的学习目标。我们再次重复计算机视觉先驱David Marr提出的设想：<strong>视觉的主要功能，在于建立环境的三维模型，并且在交互中学习知识</strong>。计算机视觉，包括其他AI方向，都应该朝着这个方向发展，以走向真正的实用。</p>
<p>总之，在不同子领域，<strong>单纯依靠统计学习（特别是深度学习）的强拟合能力的尝试，都已经走到了极限</strong>。未来的发展，一定建立在对CV更本质的理解上，而在各种任务上建立更合理的评价指标，则是我们需要迈出的第一步。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="结语">结语<a href="#结语" class="hash-link" aria-label="结语的直接链接" title="结语的直接链接">​</a></h2>
<p>经过几次密集的学术交流，我能够明显地感受到业界的迷茫，至少对于视觉感知（识别）而言，有意思、有价值的研究问题越来越少，门槛也越来越高。这样发展下去，有可能在不久的将来，CV研究会走上NLP的道路，逐渐分化成两类：</p>
<p>一类使用巨量计算资源进行预训练，不断空虚地刷新SOTA；一类则不断设计出新颖却没有实际意义的setting以强行创新。这对于CV领域显然不是好事。</p>
<p>为了避免这种事情，除了不断探索视觉的本质、创造出更有价值的评测指标，还需要业界增加宽容性，特别是对于非主流方向的宽容性，不要一边抱怨着研究同质化，一边对于没有达到SOTA的投稿痛下杀手。当前的瓶颈是所有人共同面对的挑战，如果AI的发展陷入停滞，没有人能够独善其身。</p>
<p>感谢看到最后。欢迎友善的讨论。</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zh-Hans/blog/tags/note">Note</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zh-Hans/blog/tags/seekyou">seekyou</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="博客导航"><a class="pagination-nav__link pagination-nav__link--next" href="/zh-Hans/blog/CV-Class-report"><div class="pagination-nav__sublabel">下一篇</div><div class="pagination-nav__label">计算机视觉报告</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#cv-和-nlp" class="table-of-contents__link toc-highlight">CV 和 NLP</a><ul><li><a href="#cv的三大基本困难和对应研究方向" class="table-of-contents__link toc-highlight">CV的三大基本困难和对应研究方向</a></li><li><a href="#cv的研究方向分析" class="table-of-contents__link toc-highlight">CV的研究方向分析</a></li><li><a href="#统一视觉识别和按需视觉识别的对比" class="table-of-contents__link toc-highlight"><strong>统一视觉识别和按需视觉识别的对比</strong></a></li></ul></li><li><a href="#结语" class="table-of-contents__link toc-highlight">结语</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">学习</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/zh-Hans/docs/skill">Note</a></li><li class="footer__item"><a class="footer__link-item" href="/zh-Hans/blog">博客</a></li><li class="footer__item"><a class="footer__link-item" href="/zh-Hans/docs/all-skill-intro">All</a></li></ul></div><div class="col footer__col"><div class="footer__title">联系我</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://qm.qq.com/cgi-bin/qm/qr?k=a2rVCTEuseKHmnnsacgTeZhTzk9Xqk1s" target="_blank" rel="noopener noreferrer" class="footer__link-item">QQ<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://join.skype.com/invite/ybaQadoUcuaJ" target="_blank" rel="noopener noreferrer" class="footer__link-item">Skype<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://space.bilibili.com/473511957" target="_blank" rel="noopener noreferrer" class="footer__link-item">b站<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">更多</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/cowqer" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://gpt.seekyou.top" target="_blank" rel="noopener noreferrer" class="footer__link-item">GPT(expired)<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ©2023.12 Seekyou,Built by C.Q.</div></div></div></footer></div>
</body>
</html>